{"cells":[{"cell_type":"markdown","source":["## Learning PySpark\n### Chapter 4: DataFrames\nThis notebook contains sample code from Chapter 4 of [Learning PySpark]() focusing on PySpark and DataFrames."],"metadata":{}},{"cell_type":"markdown","source":["### Generate your own DataFrame\nInstead of accessing the file system, let's create a DataFrame by generating the data.  In this case, we'll first create the `stringRDD` RDD and then convert it into a DataFrame when we're reading `stringJSONRDD` using `spark.read.json`."],"metadata":{}},{"cell_type":"code","source":["# Generate our own JSON data \n#   This way we don't have to access the file system yet.\nstringJSONRDD = sc.parallelize((\"\"\" \n  { \"id\": \"123\",\n    \"name\": \"Katie\",\n    \"age\": 19,\n    \"eyeColor\": \"brown\"\n  }\"\"\",\n   \"\"\"{\n    \"id\": \"234\",\n    \"name\": \"Michael\",\n    \"age\": 22,\n    \"eyeColor\": \"green\"\n  }\"\"\", \n  \"\"\"{\n    \"id\": \"345\",\n    \"name\": \"Simone\",\n    \"age\": 23,\n    \"eyeColor\": \"blue\"\n  }\"\"\")\n)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# Create DataFrame\nswimmersJSON = spark.read.json(stringJSONRDD)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# Create temporary table\nswimmersJSON.createOrReplaceTempView(\"swimmersJSON\")"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# DataFrame API\nswimmersJSON.show()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# SQL Query\nspark.sql(\"select * from swimmersJSON\").collect()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["%sql \n-- Query Data\nselect * from swimmersJSON"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["#### Inferring the Schema Using Reflection\nNote that Apache Spark is inferring the schema using reflection; i.e. it automaticlaly determines the schema of the data based on reviewing the JSON data."],"metadata":{}},{"cell_type":"code","source":["# Print the schema\nswimmersJSON.printSchema()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["Notice that Spark was able to determine infer the schema (when reviewing the schema using `.printSchema`).\n\nBut what if we want to programmatically specify the schema?"],"metadata":{}},{"cell_type":"markdown","source":["#### Programmatically Specifying the Schema\nIn this case, let's specify the schema for a `CSV` text file."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import *\n\n# Generate our own CSV data \n#   This way we don't have to access the file system yet.\nstringCSVRDD = sc.parallelize([(123, 'Katie', 19, 'brown'), (234, 'Michael', 22, 'green'), (345, 'Simone', 23, 'blue')])\n\n# The schema is encoded in a string, using StructType we define the schema using various pyspark.sql.types\nschemaString = \"id name age eyeColor\"\nschema = StructType([\n    StructField(\"id\", LongType(), True),    \n    StructField(\"name\", StringType(), True),\n    StructField(\"age\", LongType(), True),\n    StructField(\"eyeColor\", StringType(), True)\n])\n\n# Apply the schema to the RDD and Create DataFrame\nswimmers = spark.createDataFrame(stringCSVRDD, schema)\n\n# Creates a temporary view using the DataFrame\nswimmers.createOrReplaceTempView(\"swimmers\")"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# Print the schema\n#   Notice that we have redefined id as Long (instead of String)\nswimmers.printSchema()"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["%sql \n-- Query the data\nselect * from swimmers"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["As you can see from above, we can programmatically apply the `schema` instead of allowing the Spark engine to infer the schema via reflection.\n\nAdditional Resources include:\n* [PySpark API Reference](https://spark.apache.org/docs/2.0.0/api/python/pyspark.sql.html)\n* [Spark SQL, DataFrames, and Datasets Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html#programmatically-specifying-the-schema): This is in reference to Programmatically Specifying the Schema using a `CSV` file."],"metadata":{}},{"cell_type":"markdown","source":["####|| SparkSession\n\nNotice that we're no longer using `sqlContext.read...` but instead `spark.read...`.  This is because as part of Spark 2.0, `HiveContext`, `SQLContext`, `StreamingContext`, `SparkContext` have been merged together into the Spark Session `spark`.\n* Entry point for reading data\n* Working with metadata\n* Configuration\n* Cluster resource management\n\nFor more information, please refer to [How to use SparkSession in Apache Spark 2.0](http://bit.ly/2br0Fr1) (http://bit.ly/2br0Fr1)."],"metadata":{}},{"cell_type":"markdown","source":["### Querying with SQL\nWith DataFrames, you can start writing your queries using `Spark SQL` - a SQL dialect that is compatible with the Hive Query Language (or HiveQL)."],"metadata":{}},{"cell_type":"code","source":["# Execute SQL Query and return the data\nspark.sql(\"select * from swimmers\").show()"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["Let's get the row count:"],"metadata":{}},{"cell_type":"code","source":["# Get count of rows in SQL\nspark.sql(\"select count(1) from swimmers\").show()"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["Note, you can make use of `%sql` within the notebook cells of a Databricks notebook."],"metadata":{}},{"cell_type":"code","source":["%sql \n-- Query all data\nselect * from swimmers"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["# Query id and age for swimmers with age = 22 via DataFrame API\nswimmers.select(\"id\", \"age\").filter(\"age = 22\").show()"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["# Query id and age for swimmers with age = 22 via DataFrame API in another way\nswimmers.select(swimmers.id, swimmers.age).filter(swimmers.age == 22).show()\n"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["# Query id and age for swimmers with age = 22 in SQL\nspark.sql(\"select id, age from swimmers where age = 22\").show()"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["%sql \n-- Query id and age for swimmers with age = 22\nselect id, age from swimmers where age = 22"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["# Query name and eye color for swimmers with eye color starting with the letter 'b'\nspark.sql(\"select name, eyeColor from swimmers where eyeColor like 'b%'\").show()"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["%sql \n-- Query name and eye color for swimmers with eye color starting with the letter 'b'\nselect name, eyeColor from swimmers where eyeColor like 'b%'"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["### Querying with the DataFrame API\nWith DataFrames, you can start writing your queries using the DataFrame API"],"metadata":{}},{"cell_type":"code","source":["# Show the values \nswimmers.show()"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["# Using Databricks `display` command to view the data easier\ndisplay(swimmers)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["# Get count of rows\nswimmers.count()"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["# Get the id, age where age = 22\nswimmers.select(\"id\", \"age\").filter(\"age = 22\").show()"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["# Get the name, eyeColor where eyeColor like 'b%'\nswimmers.select(\"name\", \"eyeColor\").filter(\"eyeColor like 'b%'\").show()"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["## On-Time Flight Performance\nQuery flight departure delays by State and City by joining the departure delay and join to the airport codes (to identify state and city)."],"metadata":{}},{"cell_type":"markdown","source":["### DataFrame Queries\nLet's run a flight performance using DataFrames; let's first build the DataFrames from the source datasets."],"metadata":{}},{"cell_type":"code","source":["# Set File Paths\nflightPerfFilePath = \"/databricks-datasets/flights/departuredelays.csv\"\nairportsFilePath = \"/databricks-datasets/flights/airport-codes-na.txt\"\n\n# Obtain Airports dataset\nairports = spark.read.csv(airportsFilePath, header='true', inferSchema='true', sep='\\t')\nairports.createOrReplaceTempView(\"airports\")\n\n# Obtain Departure Delays dataset\nflightPerf = spark.read.csv(flightPerfFilePath, header='true')\nflightPerf.createOrReplaceTempView(\"FlightPerformance\")\n\n# Cache the Departure Delays dataset \nflightPerf.cache()"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["# Query Sum of Flight Delays by City and Origin Code (for Washington State)\nspark.sql(\"select a.City, f.origin, sum(f.delay) as Delays from FlightPerformance f join airports a on a.IATA = f.origin where a.State = 'WA' group by a.City, f.origin order by sum(f.delay) desc\").show()"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["%sql\n-- Query Sum of Flight Delays by City and Origin Code (for Washington State)\nselect a.City, f.origin, sum(f.delay) as Delays\n  from FlightPerformance f\n    join airports a\n      on a.IATA = f.origin\n where a.State = 'WA'\n group by a.City, f.origin\n order by sum(f.delay) desc\n "],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["# Query Sum of Flight Delays by State (for the US)\nspark.sql(\"select a.State, sum(f.delay) as Delays from FlightPerformance f join airports a on a.IATA = f.origin where a.Country = 'USA' group by a.State \").show()"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["%sql\n-- Query Sum of Flight Delays by State (for the US)\nselect a.State, sum(f.delay) as Delays\n  from FlightPerformance f\n    join airports a\n      on a.IATA = f.origin\n where a.Country = 'USA'\n group by a.State "],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["%sql\n-- Query Sum of Flight Delays by State (for the US)\nselect a.State, sum(f.delay) as Delays\n  from FlightPerformance f\n    join airports a\n      on a.IATA = f.origin\n where a.Country = 'USA'\n group by a.State "],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":["For more information, please refer to:\n* [Spark SQL, DataFrames and Datasets Guide](http://spark.apache.org/docs/latest/sql-programming-guide.html#sql)\n* [PySpark SQL Module: DataFrame](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame)\n* [PySpark SQL Functions Module](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions)"],"metadata":{}}],"metadata":{"name":"Ch4 - DataFrames","notebookId":4341522646494009},"nbformat":4,"nbformat_minor":0}
